{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FIT5149 S1 2020 Assessment 2: Authorship Profiling\n",
    "\n",
    "\n",
    "### Group information:\n",
    "- Group Number: <b>46</b>\n",
    "- Group Members: <b>Camilla Maulidina Shaquila</b>, <b>Ng Jade Kuan</b>, <b>Anish Rajan</b>\n",
    "\n",
    "### Programming Environment:\n",
    "- Python 3.7.1\n",
    "- Jupyter Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "1. [Library Import](#import)<br><br>\n",
    "2. [Dataset Reading](#read)<br><br>\n",
    "3. [Preprocessing](#prep1)<br>\n",
    "    3.1. [Batch processing of data](#prep1)<br>\n",
    "    3.2. [Generalization](#prep2)<br>\n",
    "    3.2. [Removal of unnecessary elements](#prep3)<br>\n",
    "    3.3. [Tokenization, Vectorization and Feature Selection](#vec)<br><br>\n",
    "4. [Model Development and Prediction](#mod)<br>\n",
    "    4.1. [Logistic Regression](#LR)<br>\n",
    "    4.2. [Support Vector Classifier](#SVC)<br>\n",
    "    4.3. [XGBoost](#XG)<br>\n",
    "    4.4. [Basic Ensemble Model](#bEn)<br>\n",
    "5. [CSV File Creation](#csv)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:Purple\"> Import necessary libraries</h2> <a name=\"import\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\araja\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import xml.etree.ElementTree as et\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "import os\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from html import unescape\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "en_stop = set(nltk.corpus.stopwords.words('english'))\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:Purple\">Load the dataset</h2><a name=\"read\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"train_labels.csv\")                             # load 'train_labels.csv' as a dataframe\n",
    "test_labels = pd.read_csv(\"test_labels.csv\")                        # load 'test_labels.csv' as a dataframe\n",
    "test_gender = test_labels.gender.tolist()                           # get the test labels as a list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:Purple\">Pre-processing #1</h2><a name=\"prep1\"></a>\n",
    "\n",
    "### In the cell below,\n",
    "- Batch processing is being performed on the training and testing documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 3.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# initialize 6 empty lists\n",
    "file_name = []\n",
    "tw = []\n",
    "gen1 = []\n",
    "file_name2 = []\n",
    "tw2 = []\n",
    "gen2 = []\n",
    "\n",
    "def batch_func_train(batch):\n",
    "    for file in batch['id']:\n",
    "        f_name = \".\\\\data\\\\\" + str(file) + \".xml\"                   # get the name of the file     \n",
    "        f_obj = et.parse(f_name)                                    # use ElementTree to parse the file\n",
    "        tweets = f_obj.findall('documents/document')                # get the content present within the\n",
    "                                                                    # '<document></document>' tags\n",
    "\n",
    "        # get the gender value corresponding to each file\n",
    "        gen = str(batch[batch['id'] == file]['gender'])\n",
    "        # check whether the gender is female or male, and\n",
    "        # update the variable accordingly\n",
    "        if 'female' in gen:\n",
    "            gen = 'female'\n",
    "        else:\n",
    "            gen = 'male'\n",
    "\n",
    "        # append the extracted file name, document (tweet)\n",
    "        # and gender to a dataframe\n",
    "        for tweet in tweets:\n",
    "            file_name.append(f_name)\n",
    "            tw.append(tweet.text)\n",
    "            gen1.append(gen)\n",
    "\n",
    "def batch_func_test(batch):\n",
    "    for file in batch['id']:\n",
    "        f_name = \".\\\\data\\\\\" + str(file) + \".xml\"                   # get the name of the file     \n",
    "        f_obj = et.parse(f_name)                                    # use ElementTree to parse the file\n",
    "        tweets = f_obj.findall('documents/document')                # get the content present within the\n",
    "                                                                    # '<document></document>' tags\n",
    "            \n",
    "        # get the gender value corresponding to each file\n",
    "        gen = str(batch[batch['id'] == file]['gender'])\n",
    "        # check whether the gender is female or male, and\n",
    "        # update the variable accordingly\n",
    "        if 'female' in gen:\n",
    "            gen = 'female'\n",
    "        else:\n",
    "            gen = 'male'    \n",
    "        \n",
    "        \n",
    "        for tweet in tweets:\n",
    "            file_name2.append(f_name)\n",
    "            tw2.append(tweet.text)\n",
    "            gen2.append(gen)\n",
    "\n",
    "\n",
    "# create batches of 400 records of the training data and pass it through the\n",
    "# 'batch_func_train' function\n",
    "for g, df in train.groupby(np.arange(len(train)) // 400):\n",
    "        batch_func_train(df)\n",
    "\n",
    "# create batches of 250 records of the training data and pass it through the\n",
    "# 'batch_func_test' function\n",
    "for g, df in test_labels.groupby(np.arange(len(test_labels)) // 250):\n",
    "        batch_func_test(df)\n",
    "\n",
    "d = {'File Name' : file_name, 'Tweet' : tw, 'Gender' : gen1}\n",
    "train_df = pd.DataFrame(d)\n",
    "\n",
    "e = {'id' : file_name2, 'Tweet' : tw2, 'Gender' : gen2}\n",
    "test_df = pd.DataFrame(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:Purple\">Pre-processing #2</h2></span><a name=\"prep2\"></a>\n",
    "\n",
    "### In the cell below,\n",
    "- File names are being normalized\n",
    "- Duplicate instances of Gender are being removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1.39 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# FOR TRAIN DATA\n",
    "# replace \".\\\\data\\\\\" and \".xml\" with an empty string to reflect the correct file names\n",
    "train_df['File Name'].replace(r\".\\\\data\\\\\",'', regex=True, inplace=True)\n",
    "train_df['File Name'].replace(r\".xml\",'', regex=True, inplace=True)\n",
    "\n",
    "\n",
    "# group the data by the file name\n",
    "train_df_group = train_df.groupby('File Name').agg({'Tweet': ', '.join, 'Gender': ', '.join}).reset_index()\n",
    "\n",
    "# define a function to split the string by ', ', drop duplicates and join back\n",
    "def drop_duplicates(row):\n",
    "    words = row.split(', ')\n",
    "    return ', '.join(np.unique(words).tolist())\n",
    "\n",
    "#  remove duplicate instances of the gender in each row using the 'drop_duplicates' function\n",
    "train_df_group['Gender'] = train_df_group['Gender'].apply(drop_duplicates)\n",
    "\n",
    "\n",
    "# FOR TEST DATA\n",
    "# replace \".\\\\data\\\\\" and \".xml\" with an empty string to reflect the correct file names\n",
    "test_df['id'].replace(r\".\\\\data\\\\\",'', regex=True, inplace=True)\n",
    "test_df['id'].replace(r\".xml\",'', regex=True, inplace=True)\n",
    "\n",
    "# group the data by the file name\n",
    "test_df_group = test_df.groupby('id').agg({'Tweet': ', '.join, 'Gender': ', '.join}).reset_index()\n",
    "\n",
    "test_df_group['Gender'] = test_df_group['Gender'].apply(drop_duplicates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:Purple\">Pre-processing #3<a name=\"prep3\"></h2></a>\n",
    "\n",
    "### In the cell below, the following pre-processing is being performed:\n",
    "- Removal of `hashtags`\n",
    "- Normalization of `unicode characters`\n",
    "- Removal of `mentions`\n",
    "- Removal of `links`\n",
    "- Removal of `duplicate tokens`\n",
    "- Removal of `non-aplhabetic` characters\n",
    "- Converison of tokens into `lowercase`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 4.41 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# initialize two empty lists\n",
    "train_docs = []\n",
    "test_docs = []\n",
    "\n",
    "# the loop converts the text into a more generalized format (pre-processing)\n",
    "for file in train_df_group['File Name']:\n",
    "    string_list = train_df_group[train_df_group['File Name'] == file]['Tweet'].tolist()  # get the tweets of a file\n",
    "    \n",
    "    rem_links_mentions = re.sub(r\"(?:\\@|https?\\://)\\S+\", '', str(string_list).lower())   # make tweets lowercase and\n",
    "                                                                                         # remove twitter mentions, links\n",
    "        \n",
    "    rem_hashtags = re.sub(r\"(?:\\#\\w+)\", '', rem_links_mentions)                          # remove hashtags\n",
    "    \n",
    "    convert_unicode_chars = BeautifulSoup(unescape(rem_hashtags), 'lxml')                # convert unicode characters\n",
    "                                                                                         # to their original form\n",
    "        \n",
    "    result_str = convert_unicode_chars.text                                              # get the converted text as a \n",
    "                                                                                         # string\n",
    "        \n",
    "    keep_words = re.sub(r\"[^a-zA-Z ]\", '', result_str)                                   # replace non-word characters\n",
    "                                                                                         # with an empty space       \n",
    "        \n",
    "    keep_words = \" \".join(keep_words.split())  \n",
    "\n",
    "    train_docs.append(keep_words)                                                        # append the result to 'docs'\n",
    "\n",
    "\n",
    "\n",
    "# the loop converts the text into a more generalized format (pre-processing)\n",
    "for file in test_df_group['id']:\n",
    "    string_list = test_df_group[test_df_group['id'] == file]['Tweet'].tolist()           # get the tweets of a file\n",
    "    \n",
    "    rem_links_mentions = re.sub(r\"(?:\\@|https?\\://)\\S+\", '', str(string_list).lower())   # make tweets lowercase and\n",
    "                                                                                         # remove twitter mentions, links\n",
    "        \n",
    "    rem_hashtags = re.sub(r\"(?:\\#\\w+)\", '', rem_links_mentions)                          # remove hashtags\n",
    "    \n",
    "    convert_unicode_chars = BeautifulSoup(unescape(rem_hashtags), 'lxml')                # convert unicode characters\n",
    "                                                                                         # to their original form\n",
    "    \n",
    "    result_str = convert_unicode_chars.text                                              # get the converted text as a \n",
    "                                                                                         # string\n",
    "        \n",
    "    keep_words = re.sub(r\"[^a-zA-Z ]\", '', result_str)                                   # replace non-word characters\n",
    "                                                                                         # with an empty space\n",
    "        \n",
    "    keep_words = \" \".join(keep_words.split())                                            # convert the list to string\n",
    "\n",
    "    test_docs.append(keep_words)                                                         # append the result to 'test_docs'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:Purple\">Vectorization and Feature Selection</h2><a name=\"vec\"></a>\n",
    "\n",
    "### In the cell below, Tf-Idf Vectorizer is being used with the following parameters set to the following:\n",
    "- Conversion of tokens into <b>lowercase</b>\n",
    "- Removal of <b>stop words</b> by making use of the list of stop words provided by `NLTK`\n",
    "- Removal  of words appearing in less than <b>5%</b> of the documents\n",
    "- Removal of words appearing in more than <b>95%</b> of the documents\n",
    "- Creation of <b>Unigrams</b>, <b>Bigrams</b> and <b>Trigrams</b>\n",
    "- Use of <b>TweetTokenizer()</b> to tokenize the document\n",
    "- The <b>Top 1800</b> features have been selected\n",
    "<br/><br/>\n",
    "\n",
    "Furthermore, the training data's vocabulary is being learned and transformed into a term-document matrix. The test data is being transformed into a term-document matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 30.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "tokenizer = TweetTokenizer(strip_handles=True, reduce_len=True) # use TweetTokenizer() to tokenize every document\n",
    "for idx in range(len(train_docs)):\n",
    "    train_docs[idx] = train_docs[idx].lower()                   # make it lowercase\n",
    "    train_docs[idx] = tokenizer.tokenize(train_docs[idx])       # split into words\n",
    "\n",
    "\n",
    "for idx in range(len(test_docs)):\n",
    "    test_docs[idx] = test_docs[idx].lower()                     # make it lowercase\n",
    "    test_docs[idx] = tokenizer.tokenize(test_docs[idx])         # split into words\n",
    "\n",
    "\n",
    "def identity_tokenizer(text):\n",
    "    return text\n",
    "\n",
    "\n",
    "\n",
    "encoder = LabelEncoder()                                        # get LabelEncoder()\n",
    "y_train = encoder.fit_transform(train_df_group['Gender'])       # fit label encoder and return encoded labels for Gender\n",
    "true_labels = encoder.fit_transform(test_df_group['Gender'])    # fit label encoder and return encoded labels for Gender\n",
    "\n",
    "\n",
    "vectorizer = TfidfVectorizer(analyzer='word',input='content',\n",
    "                           lowercase=False,\n",
    "                           stop_words= en_stop,                 # remove stop words\n",
    "                           min_df=0.05,                         # remove words appearing in less than 5% of the documents\n",
    "                           max_df=0.95,                         # remove words appearing in more than 95% of the documents\n",
    "                           ngram_range=(1,3),                   # create unigrams and bigrams\n",
    "                           tokenizer=identity_tokenizer,        # tokenization\n",
    "                           max_features = 1800)                 # feature selection\n",
    "\n",
    "x_train = vectorizer.fit_transform(train_docs)                  # learn vocabulary and idf, return term-document matrix for\n",
    "                                                                # train data\n",
    "\n",
    "x_test = vectorizer.transform(test_docs)                        # learn vocabulary and idf, return term-document matrix for\n",
    "                                                                # test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:Purple\">Model Development</h2><a name=\"mod\"></a>\n",
    "\n",
    "### The following models have been used:\n",
    "- Logistic Regression\n",
    "- Support Vecor Classifier\n",
    "- XGBoost\n",
    "- Basic Ensemble Model (<b>Used to generate the output csv file</b>)*\n",
    "\n",
    "\n",
    "\\* Please note that to run the Basic Ensemble Model, the Logistic Regression, SVC and XGBoost models need to run. This is why those 3 models are included in this program."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:Green\">Logistic Regression</h3><a name=\"LR\"></a>\n",
    "\n",
    "#### In the cell below, Logistic Regression is being implemented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Accuracy on the test labels is:  81.0\n",
      "Wall time: 88.8 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# load the LogisticRegression() class\n",
    "LR_model = LogisticRegression()\n",
    "\n",
    "# fit the training data to the model\n",
    "LR_model.fit(x_train,y_train)\n",
    "\n",
    "# use the model to predict the labels of the test data\n",
    "pred_LR = LR_model.predict(x_test)\n",
    "\n",
    "# get the prediction of the test labels\n",
    "acc_test_LR = accuracy_score(true_labels, pred_LR) * 100\n",
    "print(\"The Accuracy on the test labels is: \", acc_test_LR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:Green\">Support Vector Classifier</h3><a name=\"SVC\"></a>\n",
    "\n",
    "#### In the cell below, Support Vector Classifier is being implemented with polynomial kernel and the regularization parameter set to \"2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Accuracy on the test labels is:  78.0\n",
      "Wall time: 21.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# load the SVC() class\n",
    "SVC_model = SVC(kernel='poly', degree=2, C=7)\n",
    "\n",
    "# fit the training data to the model\n",
    "SVC_model.fit(x_train,y_train)\n",
    "\n",
    "# use the model to predict the labels of the test data\n",
    "pred_SVC = SVC_model.predict(x_test)\n",
    "\n",
    "# get the prediction of the test labels\n",
    "acc_test_SVC = accuracy_score(true_labels, pred_SVC) * 100\n",
    "print(\"The Accuracy on the test labels is: \", acc_test_SVC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:Green\">XGBoost</h3><a name=\"XG\"></a>\n",
    "\n",
    "#### In the cell below, XGBoost is being implemented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Accuracy on the test labels is:  75.8\n",
      "Wall time: 4.02 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# load the xgboost class\n",
    "\n",
    "\n",
    "XGB_model = XGBClassifier(booster='gbtree', eta=0.05, reg_lambda = 0.1)\n",
    "\n",
    "# fit the training data to the model\n",
    "XGB_model.fit(x_train, y_train) \n",
    "\n",
    "# use the model to predict the labels of the test data\n",
    "pred_XGB = XGB_model.predict(x_test)\n",
    "\n",
    "# get the prediction of the test labels\n",
    "acc_test_XGB = accuracy_score(true_labels, pred_XGB) * 100\n",
    "print(\"The Accuracy on the test labels is: \", acc_test_XGB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:Green\">Basic Ensemble Model</h3><a name=\"bEn\"></a>\n",
    "\n",
    "#### In the cell below, Basic Ensemble Model is being implemented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Accuracy on the test labels is:  81.8\n",
      "Wall time: 2min 23s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# use ensemble method from previous highest models\n",
    "ensemble_model = sklearn.ensemble.StackingClassifier(cv = 6, \n",
    "                                                     estimators=[('lr', LR_model), ('svc', SVC_model), ('xgb', XGB_model)])\n",
    "\n",
    "# fit the training data to the model\n",
    "ensemble_model.fit(x_train,y_train)\n",
    "\n",
    "# use the model to predict the labels of the test data\n",
    "pred_ensemble = ensemble_model.predict(x_test)\n",
    "\n",
    "# get the prediction of the test labels\n",
    "acc_test_ens = accuracy_score(true_labels, pred_ensemble) * 100\n",
    "print(\"The Accuracy on the test labels is: \", acc_test_ens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:Purple\">CSV Creation</h2><a name=\"csv\"></a>\n",
    "\n",
    "#### In the cell below, a CSV file titled \"pred_labels.csv\" is being created from the prediction results generated by the Basic Ensemble Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace the values of the 'Gender' column with the values of 'pred_ensemble'\n",
    "test_df_group['Gender'] = pred_ensemble\n",
    "\n",
    "# keep only the 'id' and 'Gender' columns\n",
    "test_df_group = test_df_group[['id', 'Gender']]\n",
    "\n",
    "# rename the columns\n",
    "test_df_group.columns = ['id', 'gender']\n",
    "\n",
    "# export the dataframe as a csv file\n",
    "test_df_group.to_csv(\"pred_labels.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
